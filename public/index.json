[{"content":"","date":null,"permalink":"/","section":"Cycles Of Reinvention","summary":"","title":"Cycles Of Reinvention"},{"content":"","date":null,"permalink":"/tags/diet/","section":"Tags","summary":"","title":"Diet"},{"content":"I was chubby all my life. Long time ago, I accepted it as part of my identity. I was unable to loose weight even when exercising and spending most of my free time outside chasing ball. All my years at school I was overweight and unable to buy nice clothes as I was out of confection size. But this is not story about how hard it was then. You can image \u0026hellip; I was target to bullies, xenophobia, point fingering and minimal support from family as my father wanted slim, football son that I could never be. No, this is story about how I get out of obesity, about pitfalls and how to finally loose 50kg of weight and manage to stay on same weight for years. This story can be very beneficial for you if you want to loose weight or you know somebody in your family or friends who struggles with weight or you are just curious. Either way my journey persuaded me that maintenance of stable weight is topic for everybody. Obesity, type 2 diabetes is so prevalent, that soon or later it will happen to you that your doctor will tell you, you need to lose weight as you are on a good way to die on CVD - heart attack. (If not, you will be one of few, genetically lucky ones)\nThe same happened to me years ago when I have visited my practitioner. I was obese, single programmer spending too much time at work, sitting on chair, drinking energy drinks, eating snacks, liters of coffee with milk. I was workaholic. I spent nearly all my day at work coding or browsing internet how to do my job better. I was true greenhorn, looking for the best design patterns and silver bullets. I always loved to be praised by my senior colleagues for great work. I was the great material to burn out. And surely I did.\nI come to revelation at one long night when I was working on side project at home. At that period of time I managed to work all day at work, then at night walk back home and work whole night through for hobby project. I was eating my late second dinner after midnight. Thinking non-stop requires calories and sure I was binging a lot\u0026hellip; One such evening I was sitting behind table and looked down to my belly. I could see just obese body and I start wondering how I end-up here. I was always chubby but not obese. Somehow I decided at that moment that problem with weight is an actual problem.\nSo I started with gold standard \u0026ldquo;eat less and move more strategy\u0026rdquo;. I have picked swimming for 1h. I was too obese to do anything else. I started swimming every second day. Determination was never my problem, fighting with bullies at school made me pretty resilient and strong minded. If I consider problem a problem I would try always my best. With such frequent exercise, one day morning my trousers just fall of me and I spotted my first visible change.\nWith couple kilos down I could finally start doing other sports that I was used to do. I started with running. I never liked sprints but I like endurance sports. I started slowly 3km, 4km, and end-up with regular 8 km run after few months. I also continued with swimming. It was actually very powerful combo and it started to be very, VERY visible.\nMy old pajama pants end-up as floor rag. I end-up to be in best physical form of my life. Actually regular sport decreased number of night hacking shifts as my motivation changed and therefore my sleep improved a lot. I started to be very energetic. I loose craving for sugar, for energy drinks and liters of coffee. It pumped my motivation and I started joining organized runs. I started to have a problem with clothes. I went from tight XXL to very comfortable L. I got feeling that I am back \u0026hellip;, finally.\nIf we end-up here it would be only half of story and it would look like a clickbait. Life is more complicated than news headings. I learned that hard way \u0026hellip;\nLosing weight is easy, weight maintain is the true art.\nMy weight roller coaster started simple I got in love ‚ù§\u0026hellip;\nThere is nothing unusual with getting in love other than your priorities will change. And exactly that happened to me. My spartan life ended. I started solving other more urgent matters. There was always something more important than simply get outside running. I wanted to spend as much time with my wife so I could do casual thinks like watch films, dinner together, visit family members, feasts. We moved to new location and started living together. In time we got two wonderful kids. My regular sporting habits land in ruins. Though I did not stop it completely. I attend swimming in local swimming pool and started running in nearby hills. But it was very rare compared to nearly daily sporting before and I was getting more chubby again. I thought and I still think it was a good compromise. We got newborn babies and it takes two adult people handle them. I needed to tandem this ride with my wife. That is what dads do.\nThe coffin in nail was COVID-19 and lock down. I stayed at home-office for many months and my weight was rock-climbing, again. May weight was once again over 130kg, my trick with swimming was not possible as everything was closed and I was too heavy for running. I tried to exercise yoga with my wife but I was so obese that I could not hold that habit. Yoga for obese is actually hard.\nLater I had to pick my old oversized clothes from old box and put my lean shirts back into box with groan that one day I will take them back. I was depressed, I was feeling as a complete failure\u0026hellip; I knew I need to loose weight, change something but at that time I did not know where to start. As many obese people like me I was clueless. I got determination to change, as I am fighter from childhood but I felt so powerless.\nme, over 130kg. Then happened something totally unexpected \u0026hellip; I got lucky in my life.\nYoutube news feed algorithm showed me a video showing American doctor Dr. Pradip Jamnadas doing lecture about intermittent fasting and ketogenic diet. In nutshell his advice was eat once a day diet specifically triggering your ketone bodies, low-carb diet. It should help you to decrease weight just changing what you eat. For sure he was quack, I thought. Nevertheless he said you can easily measure ketones in your morning urine, just buy urine strip in pharmacy. So I gave it a try. I bought strip and ate once a day (a confit duck for starter :)). Next morning I sampled urine and I could measure a decent amount ketones.\ndecent number of ketone bodies in my urine measured at first day of diet by an urine strip. It worked ! I was so amazed with this small success. It gave me a kick to continue eating once a day for next months. And I did not know how deep was this rabbit hole and where it would end-up. I continued to fast for next 7 month and I lost 50 kg without any heavy workout, without going to jim (closed because of pandemic), mostly sitting in chair at home office, working in garden and loosing weight. It fit what I could do during covid and it worked perfectly.\nfrom 138kg back to 90kg after 7 month of fasting \u0026hellip;.\nIs there any continuation to this ? Actually \u0026hellip;. there is\nI was so amazed how this is possible that I spent all of my time during covid chasing down this rabbit hole. My wife also joined researching along the way. She is a MD and she was also surprised. Actually nutrition was not a subject in MD curriculum at her time attending university. We read scientific papers and learn ton more. Most of papers is current research and there is so much more science in fasting then just getting hungry. One of effects is that your body will decrease secretion of ghrelin and neuropeptide Y that leads to less hunger, less cravings, less calories in. Second is that during ketogenic diet insulin level drops to levels where your body start to prefer burning fat over everything else. Low insulin spikes inhibits cravings for food. Very convenient when you want to effectively loose 50kg!\nBlue dots - my weight. Caloric restriction (eats once a day, fully) and ketogenic diet did a trick. Red dots - ketone bodies. I sampled data nearly every day. To the other hand standard western diet is full of carbs. Carbs are added nearly everywhere. If you do not take a look to every package label you can be surprised how much added sugar you would get in. How much calories is in chips and nuts. You will burn them out if you train often, but expect yoyo effect when you stop.\nSame happened to me years ago. I was unprepared and unaware of many details regarding nutrition. What are calories? Is there any difference in metabolism of sugars, fats and proteins? Can food program your genes to boost lipogenesis in a liver ? What exactly happens in your body after you eat ? How to build your schedule so you can maintain weight for long time ? How to setup your mind to resists obesogenic environment we live in ? How to loose weight effectively - so you can actually maintain weight for years ? The answer is more deep than missing knowledge in biochemistry. It is also social problem. It is very hard to resist food. Food advertisement is everywhere. We are also used to calm our stress with food. All this can lead to vicious circle \u0026ldquo;in stress -\u0026gt; binging -\u0026gt; be in stress from binging -\u0026gt; more binging\u0026rdquo;.\nWith my wife we realized that answers to how to manage weight effectively is a great content to share. Good things must spread\u0026hellip; We started a Youtube channel - Metabolicky ridicak (only in Czech) with content we learned about dieting. We see many people around us suffering with diabetes, obesity, high cholesterol. With no good way how to actually start. What food can they actually cook. What is needed for effective fasting? Channel contains short videos with quick recipes. Deep dive videos to bring important details from metabolism, reviews of diets (that we tested) to kick start dieting and our own pitfalls that can help persuade viewers that \u0026ldquo;problem is a problem that should be targeted effectively\u0026rdquo;.\nAll materials/videos are open and free, as open source / open science. As it should be \u0026hellip;\nThere is already fruit coming back to us. My pre-diabetic mom already lost 14 kg. She was able to change her diet even though she is conservative to food. She did it gradually and found the diet working for her. And not opposite - working for diet. Now she even find a time for regular exercise !\nMy journey persuades me that even our biggest weakness can be turned to advantage. Yeah, it can take some time and pitfalls and looping in circles but that is normal. We are just people. There is no silver bullet diet and all \u0026ldquo;good advice\u0026rdquo; needs to modified to particular context. To every ones preference, lifestyle and limits. Only then it will be effective. When short term diet will turn to long term habit.\nAll best to who ever starts weight loss endeavor \u0026hellip;.\nMy journey continues \u0026hellip; ‚õµ\nPeter\nI am back! and running ! With my daughter Elis ","date":"23 November 2023","permalink":"/posts/hackers-diet/","section":"Posts","summary":"","title":"Hacker's Diet"},{"content":"","date":null,"permalink":"/tags/lifestyle/","section":"Tags","summary":"","title":"Lifestyle"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/element.io/","section":"Tags","summary":"","title":"Element.io"},{"content":" TLTR; I had deleted my Facebook account. Ciao.\nNot much more to add. I asked for an account delete. I had my reasons. Facebook provides amazing service but it comes with a price. And the cost is your privacy. With your first log-in every photo is analyzed. Every message is read by an A.I. and even every your move outside Facebook is tracked. That all together is the new currency of 21st century. Your behavior, opinion on matter, your plans and desires. Thats what we used to call privacy. Currently privacy is ignored. Its is considered as obsolete relict and also hardly to be exploited. It is not visible, so easily ignored. But it is so important! Over 90 % of our time we act according to feelings.\nDid you ever wonder why ads on web-pages can be so precise ? You are tracked. Did you realised that prices on Amazon are different for you and different for your friends? Sales uses psychological profile to determine margin and price per customer. Even in president elections. Trump used psychological profiles of USA citizens in his last campaign. Pro-Brexit party used psychological profiles to win referendum in GB. Read more about Cambridge Analytica if interested.\nCurrently new regulations about data privacy GDPR is enforced and I welcome it because this field needs it. I consider this regulation of protection of your data. Nobody cared until now. IT field is not mature as other branches of business. Not many regulations, so it can take advantage of its full potential. But for my opinion it took too much. It made privacy obsolete. I do not expect to be my letters opened when I send them through PPL or DHL. They do not open packages and scan them so they can sell those informations to third party partners. Do you care if they open packages? If yes, why do you not care about Facebook also. I leave this question unanswered and let readers to decide by themselves.\nMy mind is clear in this matter. I have decided to leave Facebook due to its \u0026ldquo;no privacy\u0026rdquo; policy. Privacy matters !\nBut I do not wish to cut off my contacts. So I decided to host my own chat service where I gradually ported my family members and all important contacts. I wanted to share here details \u0026ldquo;why, what and how\u0026rdquo; I did it. Why\nMy \u0026ldquo;Facexit\u0026rdquo; idea started a few months ago. I had my \u0026ldquo;the last nail in the coffin\u0026rdquo; moment when I read about Cambridge Analytica.\nPropaganda works best, when you do not know its source. Facebook delivered what propagandists have always wanted, a complete blurring of the line between editorial and advertising, often delivered with the added reliability of having been ‚Äúshared‚Äù by a ‚Äúfriend‚Äù. 1\n\u0026hellip; figuring out whether a particular voter was, say, a neurotic introvert, a religious extrovert, a fair-minded liberal or a fan of the occult. Those were among the psychological traits the firm claimed would provide a uniquely powerful means of designing political messages. 2\nWhat I consider as fundamentally broken is the monopol\nyour identity is Facebook identity - you can have only a single account and communicate only with other Facebook accounts, You do not like Facebook policy, you can still leave, but you will leave also all your contacts. information sources - your wall is build by Facebook alghorithm. You can pick and choose your activities, follow groups but you do not have much choice how much \u0026ldquo;other content\u0026rdquo; - adds, propaganda, fake-news you will see. Building personal source of information I consider one of my basic rights to be well informed. I spend a lot of time to find proper sources. You can leave Facebook, but once again, you will loose your information sources. Is a choice from one option still a choice ?\nThis is a fundamental question. Do you want to leave inside a golden cage with a golden path inside or not ? Do you want to embrace the truth or leave in a fable. I do not expect that everybody bothers, I do not expect to those who bothers will react eventually. But I want to give to those who are willing to listen all what I have learned on my way and let them choose. To give them more options then one.\nMy job is to work with data, I work with algorithms that analyze you, make predictions about you and bring benefits to life of everybody. I am familiar with what it really takes to act. I can help in this case. So why shouldn\u0026rsquo;t I ?\nWhat next \u0026hellip; #I have started this project some months ago where I was looking for more proper alternative to Facebook. My requirements were (all must):\nsimple - app has to be simple and comfortable to be used by my friends and family. My mama has to feel comfortable using it, otherway no point to switch if nobody wants to message you ;-) messaging - be able to send simple messages, images, video video conferencing - support for video calls. ssl - support SSL everywhere and end-to-end encryption self hosted - data has to be stored on my servers. Otherway there is no point to switch from Facebook. You make your communication private again by storing history outside internet vendors. I want to store it securely on my own servers. I have looked to multiple projects (ejabberd, rocket, \u0026hellip;) and was unable to deliver all requirements until I have found project matrix. What really strikes me on the first sight, it is not a client or server but protocol specification. And in its core it is federated protocol. Then I realised that this is what actually solves problem with monopol. Idea of federation is simple and resembles email. You may have an email account at gmail but you can still communicate with other people in different networks, like yahoo, aol, seznam, your company , because your identity is not owned by particular network but can be shared between them. If you want to switch from your email vendor, you create another account. You do not loose your contancts. You just send them your new email address. It is simple.\nMatrix is a similar concept. It also have user address - chat address. In my case it is:\n@peter:petercipov.com\nIt has domain part (petercipov.com) and user identifier (@peter) as you have in email. So when you want to contact someone you need his chat address. Simple.\nMatrix is federated protocol. Same as email. You can use vendor hosting (f.e. matrix.org provides free chat hosting) or you can host it by yourself as I do.\nYour data is stored on homeserver and when you wish to contact somebody on different homeserver a copy of a message is transfered. Again same semantics as email.\nYou can make voice \u0026amp; video calls. Matrix supports WebRTC. It is possible to call to different device or even to web browser.\nMatrix natively support end-to-end encryption[1, 2] It can be setup per room. When enabled, messages are encrypted on client, before they are sent to server and that way they are visible only to recipients.\nMatrix is a protocol. It provides back-end reference implementation written in python and already there exists multiple front-ends, from those I like the most Riot.im that can be installed nativelly on iOS and Android or used as electron based desktop app or as standalone single-page web app.\nElement.IO user interface. How #So I have decided to host chat service myself. My use case is purelly personal. I do not want to host it for money (aka I pay the bills :)), I do not process any data (whole purpose of this project). First step was to study GDPR. I have read GDPR regulation to find what requirements I need to fulfill. I was happy to find following 1 \u0026hellip;\nThis Regulation does not apply to the processing of personal data: \u0026hellip; by a natural person in the course of a purely personal or household activity;\n100% comply :)\nEven more guys from matrix takes this regulations seriously and will provide patches to make matrix complement with GDPR 2. So for commercial purpose it will be delivered in next few patches.\nSelf Hosting #For my purposes I want to have everything simple and minimal. So cost/value ratio would be in my favour. I have bought two small 1GB RAM VPS (virtual private services). 1 for hosting of homeserver and one for hosting STUN/ TURN server userd for Voip and Video calls. Also domain DNS record is required. One for matrix homeserver chat.petercipov.com and one for STUN/TURN server turn.petercipov.com. Actually TURN server is pretty heavy on traffic and resources therefore 2 VPS.\nI use latest debian on all instances and using public/private keys to access. No passwords. All proceses I use are dockerized.\nhttps://hub.docker.com/r/petercipov/synapse-docker/tags/ for synapse homeservers https://hub.docker.com/r/petercipov/coturn-docker/ for STUN/TURN server https://hub.docker.com/r/_/postgres/ (alpine version) as data storage Actually you can use official synapse docker image with everything wrapped inside, but I prefer to use separate process and potentially scale and reshuffle instances by myself. Nevertheless all in one package is much easier to start with.\nFiles and images are stored directly on file system. I have setup size monitoring, so when they reach max limit I will backup/purge them. It is possible to use different options but for my purposes it is far from its limits.\nSSL is handle by lets encrypt and cerbot docker image.\n1 2 3 4 5 6 7 8 docker run \\ -v `pwd`/data/ssl:/etc/letsencrypt \\ -e distinct=true \\ -e domains=\u0026#34;domains.....\u0026#34; \\ -e email=\u0026#34;email....\u0026#34; \\ -p 80:80 \\ -p 443:443 \\ --rm pierreprinetti/certbot:latest Conclusion #Do not be fooled by falacy of free services.\nThe whole point with privacy, is about ownership and control. It is an asset as your finances, houses, flats. You can hire finance advisor to help you, you can rent house. But you can also say no and terminate it. Before GDPR you did not have much choice with control over your privacy. You were dragged to this bussinees even thou you did not want to. By pressing \u0026ldquo;allow access to your mobile contacts\u0026rdquo; you send all data of your friends and precreate hidden accounts for them, even for those who does not have one.\nI consider GDPR a positive step towards giving us more power over privacy. If you sell it, your choice. In matter of privacy I rather play cautious and build my own game rules and shape my life as I like it. And yes it is a lot of extra work, but believe me it is worthy.\n","date":"8 June 2018","permalink":"/posts/i-have-deleted-my-facebook-account/","section":"Posts","summary":"","title":"I have deleted my Facebook account, what now"},{"content":"","date":null,"permalink":"/tags/messaging/","section":"Tags","summary":"","title":"Messaging"},{"content":"","date":null,"permalink":"/tags/java/","section":"Tags","summary":"","title":"Java"},{"content":"","date":null,"permalink":"/tags/traces/","section":"Tags","summary":"","title":"Traces"},{"content":"One problem you can encounter when you are making distributed system is how to log properly. To be more precise, how to make your logs useful ? In case of simple application shown on every tutorial blog, logs are not mentioned, or skipped. But on the other hand they are irreplaceable when you want to figure out why is your system failing. As my colleague often speaks:\nThere is never enough of logging.\nThere are tons of frameworks to help you with it (slf4j, log4j, ‚Ä¶) but in my opinion they deliver only half of the solution. Systems in bigger scale tends to have lots of logging messages. But you also have multiple machines with multiple services, all this messages are written to one giant file (or to set of giant files) and your task is somehow to find a needle in it. It is trivial task but with bigger and bigger log files it takes more and more time. One would save a lot of trouble if all messages that are bound together could be easily retrieved from files. And that is the essence of tracing. You mark somehow long messages together and give them order by occurrence. For example you can group messages by request. All messages about database communication, your service logic messages, like ACL, custom business logic, all in a single sequence of log messages ordered by occurrence in a history. It would be really handy if logs would look like this:\nAPI request started (path: /api/user, scope:all ) Authorising session (session id: hgty7) Authorisation granted for reading User service is loading users Users loaded from database (count:452) Transforming to JSON Response sent successfully You can trace message, parameters and exceptions. When you log them separately they have less meaning opposite to when they are presented together. You have to process logs to actually obtain this kind of value and often it takes enormous amount of time. Sometimes you do not have even enough information in logs to group messages reliably. It is often a case in asynchronous programming, where request is not processed by single thread so it does not have single stack trace. Request is divided to sequence of sub-tasks that are executed in order but we cannot say anything about their actual execution time, because they are planed by system scheduler, therefore non-deterministic. You have to add additional information to message log, some kind of id, to be sure. This way you are making custom traces.\nWhat is wrong with logging ? #The core problem with logging is the loss of information and context. In the program you have full context. You know where you are, what are your inputs, what are the next steps. You log it to file in some text format. Often you do not log there all context information, because you do not think it is important. Sometimes cause of failure is a single exception from database or null pointer. Then logging single message is sufficient, but what about semantic errors, when bug is hidden in complicated flow of messages between your services? In case of distributed systems you can encounter a different kinds of failures f.e. networks floods. Then it would be handy to know what was the cause. It could be also error or some glitch in code semantics. Those kind of errors are nearly impossible to reproduce. And it is almost too late to add more logging when they occur - your system is crumbling and you should react as soon as possible.\nYour logs are stored somewhere. It may be kibana, graylog, ‚Ä¶. Those systems provide very limited searching - no joins in searches as we are used from SQL database, because such searches are not scalable. We should store already aggregated data in such systems where single message input is the aggregation of messages from single request or configuration change or system start-up or tear-down. These aggregations are custom to every project even though they share some similarities.\nIt is handy to log everything. all input parameters (if they are reasonably small, or their sizes or their ids), all computation steps in business logic, all its results, and all its output actions (sending message or responding to request). All of this information is then serialized to text format and stored in logging system.\nExample #Lets start with single node example. Call remote API that purges user history. Here is pseudo-code\n1 2 3 4 5 public int purgeHistory(Trace trace, String userId) { checkACL(trace, userId); purgeUserHistory(trace, userId); return respondOK(trace); } To purge users history, users access rights are checked, then history is purged and success response is sent back.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 private void purgeUserHistory(Trace trace, String userId) { Event purgingEvent = trace.start(\u0026#34;Purging user history (userId)\u0026#34;, userId); try { callRestService(trace, 1, \u0026#34;{\\\u0026#34;data\\\u0026#34;: \\\u0026#34;remove\\\u0026#34;}\u0026#34;, true); } catch (Exception ex) { trace.event(\u0026#34;retrying purge call\u0026#34;); ..... } purgingEvent.end(); } private void checkACL(Trace trace, String userId) { .... trace.event(\u0026#34;User has sufficient privileges (userid)\u0026#34;, userId); } private int respondOK(Trace trace) { .... trace.event(\u0026#34;Responding OK \u0026#34;); return 200; } private String callRestService(final Trace trace, final int id, final String data, final boolean fail) throws Exception { Event callEvent = trace.start(\u0026#34;calling remote service (id, data)\u0026#34;, id, data); String code; try { if (fail) { code = \u0026#34;500\u0026#34;; throw new IllegalStateException(\u0026#34;HTTP 500\u0026#34;); } else { code = \u0026#34;200\u0026#34;; } trace.event(\u0026#34;call succeeded (response)\u0026#34;, code); } catch (RuntimeException ex ) { trace.event(\u0026#34;call failed\u0026#34;, ex); throw ex; } finally { callEvent.end(); } return code; } The Trace interface is actually very simple code. It contains two methods. Start when you want to record some event that has the duration in history. Event is marked as finished when end method is called. The second method is used for simpler case when you want to record an event that has occurred at single point in time.\n1 2 3 4 5 6 7 8 public interface Trace { interface Event { void end(); } Event start(String name, Object ... values); void event(String name, Object ... values); } Example of the trace. Every action that was made in code was recorded. Every record has timestamp (dt - delta time, number of milliseconds from first event), id that is unique in trace and growing according to an execution order. It contains name of the thread (th) that has executed the code, message (n), values and exception stack traces. There some messages that refer to previous records (endOf) and they serve to note when method finished.\nAmount of data #On the one other hand it produces a hell of data. It is a feature of traces that you have mitigate and prepare your custom logic to resolve it. In a case of high frequent calls you can sample logging and pass NOOPTrace instead of logging race, and log just some percentage of them. Even such portion of data can have actual value for you. You can look for calls with high spikes. Or you can log traces only in case of exception. You would loose real-time metrics about your system but even such trace would be more useful than simple stack-trace. It really depends what you need from traces.\nNot a new idea at all #This idea, is not new. I have not invented it. It is just handy and straightforward and I have great experience with them when I wanted to figure out \u0026ldquo;why it does not work\u0026rdquo;. It is core idea behind google tracing system Dapper and Zipkin from Twitter. I have not used Zipkin because it is focused only on duration of method and I actually wanted to log some portion of data. When I was researching Zipkin it missed this important feature. In my best knowledge it does not have this feature today. Therefore I have created simple stupid library to do what I actually wanted. To the other hand Zipkin logging via scribe and processing via collectors is very sophisticated. I do not want to compete it. I think developers that created it have done a great job. But for my environment it was overkill.\nIn my case I have used tracing for application written with RxJava. Code was mostly written in functional style. Business logic are sequences of timers and waitings for responses from different services. It is highly asynchronous code. Before traces, when something had failed, it was nearly impossible to reason why. There was no complete flow, just stack trace from executor thread. And this is core idea, tracing is in some manner your custom stack with reasonable params, it just depends on you what is put inside and it is passed during whole execution of single request.\nExample of the flow. In my case service had 3 threads. One for IO, one for all event logic and timer thread. Typical request would be watching some resource for change. Request is registered and than waits for change. Thread is not blocked but request is packed as task to timer executor with some timeout. After this timeout, request is recovered and \u0026ldquo;no change is returned\u0026rdquo;. Otherwise when some modification happened meanwhile timer is canceled watch return \u0026ldquo;change occurred\u0026rdquo;. I would like to somehow log this whole process. Why request was postponed why it was recovered, which request has done it.\nWhat is the difference? #My point is that I have written Traces in much simpler manner than the other solutions. I do not assume anything at front. It is as explicit as possible. It does not need any annotation, any thread locals any request contexts, any meta-programming, just no magic. This may seem in contradiction with common approach like aspect oriented programming, where you you write a set of selectors and code is somehow automatically instrumented. My argument is that any automation will not bring you satisfying result. You have to decide what is important to you at line level. Any automation will bring you next level of abstraction that you have to reason about and I argue that meta programming approach will be un-maintainable because code contains just too many edge cases and in time they have tendency to spread.\nWhy not to go with approach of slf4j where you have logger factory that is static accessible. i.e. globally? Then you do not need any extra parameter in your methods\u0026hellip; Such approach is optimal for standard java code, where in all methods in a program are managed with the same thread. Therefore context is same. But in a case of switching between multiple threads you loose your context and what is even worse it can be \u0026ldquo;reused\u0026rdquo; with other request. You may argue that you can use some kind of token, but now you end up in same situation. You have to pass something to your methods to bind them together, why not just pass them trace object and you do not need any reflection magic and code does what is actually written in it.\nThere is a simple rule what to trace and what not. Trace something, that has the end in finite time, preferably in short time. Infinite algorithms would have infinite traces, and long lived tasks would have huge traces. You do not want flood your memory with something endless, don‚Äôt you ?\nAre traces distributed? #Simply yes and not :) You have to make them distributed.\nDistributed trace. Lets have a simple example where you have balancer nodes(B), processing nodes (P) and Cassandra nodes (C). When request (1) arrives to this infrastructure it is routed by internal balancer logic and passed (2) to a processing node. Here it is processed again and call to Cassandra is executed(3) and than response returned back to client. To trace this calls you have to pass some kind of global trace id through layers so on each each node you log separate trace but marked with this global trace id so then afterwards you can just load all traces with the same global trace id. You generate this global trace id at the entry and than just pass it to subsequent layers. It is just same idea as passing trace objects via your code.\nIs it a \u0026ldquo;new blue\u0026rdquo; ? #Traces are not something new, it is just different approach of logging. The question is do you need them? When you have distributed system with complicated set of messages that are sent between services I recommend you to consider them. You would have to figure out what amount of data you can handle and modify your code. It is some amount of work, but again any automation and general solution with aspects may not bring satisfaction. Even I am not sometimes sure from the beginning what to trace but when code starts to behave strangely I add more traces. Now I am used to use them everywhere but it may not be your case. As always, there is no silver bullet. It is just a good aid when you are stuck with the same sets problems as I have.\nYou can find sources here\n","date":"17 December 2015","permalink":"/posts/traces-sane-logging-in-async/","section":"Posts","summary":"","title":"Traces - sane logging for asynchronous code"},{"content":"","date":null,"permalink":"/tags/assumptions/","section":"Tags","summary":"","title":"Assumptions"},{"content":"","date":null,"permalink":"/tags/microservices/","section":"Tags","summary":"","title":"Microservices"},{"content":"Micro-services is buzzword of today. It is the new best, something you should definitely implement. No! Never believe in such fancy statement. Engineering is never sunshine and roses. You should be able to reason about problems and choose best for you eventhough it is not popular at the moment.\nWhat is the monolith ? According to dictionary is a statue build from single peace of stone. In our case we can imagine a classic type of application written in a single language, stored in single repository, executed in a single process, using single database, with single view model. Most monoliths have in common\ncoupled features - one feature cannot exists without other spaghetti code - there are no clear boundaries, code is fuzzy, illogical, full of weird names like Provider, Factory, Manager, Abstract, Executor. global mutable shared state is a superstar. no decomposition These are not really disadvantages of monolith, but these situations are just very common, because it is so easy to do them. And when deadlines are near it is almost a rule to bend something, to adjust something, just to make it work !\nExample of the monolith. We can imagine monolith as a graph where nodes are functions, classes, libraries, packages, data storages, tables and edges are the dependencies between them. This image is an example of coupled monolith, it is nearly full graph, where everything depends on everything. This is a typical nightmare when you obtain code after your colleague, you are not even able to understand such a beast :) and this one was maintained for decades. I have seen projects where it took months for developers just to read over codebase and understand what it does. I do not need to tell you how much joy brings such work.\nWhat will happen when your project starts to grow? Number of nodes and edges will also grow. Such project requires more man power and so you hire more people and now there are multiple of you that can change something. Just to change public interface of single node you will have to repair all of its dependencies and situations becomes even worse when change will \u0026ldquo;bubble\u0026rdquo; to multiple level of the graph. Changing something is nightmare and even worse in time the graph will grow so does the complexity.\nDoing optimisation is sometimes impossible. Lets imagine you have single table where you store structure and data. Structure is for example tree structure where every table row has a pointer to its parent and one huge blob of data. Write optimal SQL for such table is nearly impossible. It really does not matter what kind of database storage you use. You want structure be cached in RAM, but big blobs flush it and make it less effective. What you really need it is divide structure and data and optimise storages just for them.\nTo be honest, to the other hand monolith is so easy to start with that it is almost no-brainer for small projects in their early stages. In this state you do not even know what your project will look like. Leave micro-services for later and focus on business value instead.\nIf you don‚Äôt end up regretting your early technology decisions, you probably over engineered‚Ä¶‚Ä¶Re-architecting is a sign of success; if you never need to, either you overbuilt or nobody cares. (Randy Shoup)\nLots of projects starts as monoliths and monoliths is just fine for them for very long time. But then comes a time when new features requires something that monolith is not able to provide without hacks. For many cloud services it is hight availability. In simple words you cannot just shutdown your service, upgrade all binaries, run upgrade of the database over night and pray so at the morning everything will work. You have to chop monolith to set of services and upgrade them separately, so when you shut down something you shut down partially or in better case you just degrade responsiveness. Upgrade can be managed and that is the core idea. You are in control which part is upgraded and you choose proper steps and order in which they are applied.\n1 2 3 4 5 6 7 8 eBay Monolith in Perl -\u0026gt; Monolit v C++ -\u0026gt; Java -\u0026gt; microservices Twitter Monolith in Rails -\u0026gt; JS / Rails / Scala -\u0026gt; microservices Amazon Monolith in C++ -\u0026gt; Perl/C++ -\u0026gt; Java/Scala -\u0026gt; microservices All internet giants are now running clusters of micro-services. They have started as monoliths. They were written mostly in a language that was favourite one of company founder. In time, monoliths were rewritten to better structured monoliths and finally chopped to ecosystems of micro-services. What become important was the possibility for services to communicate easily. This was the reason for creation of protocol Facebook Thrift, Google Protobuff and REST. Services written Python can easily communicate with services written in Java or C.\nExample of the monolith chopping. The solution to problems with monolith is quite trivial. Chop it to services and manage them with version control system. Just like we are used to with maven, osgi, npm, the same approach was reused for micro-services (here comes handy docker and its image versioning). Every micro-service is responsible for single function, for its data and upgrades. Micro-service does just one thing( just for registration, just login, just authorisation or just comments). What you will see in time is gradual chopping. You start with single service and gradually you chop. You can chop literally till you drop. :)\nTop-down design is the rule of thumb when you do so. You start from your requirements and divide them to smaller parts until you provide optimal solution for them. The technology stack should be the last thing you choose. If you need nosql storage fine use it. If mysql is enough fine use it. But do not do it in reverse order, I have mysql and php, drupal and how would I write that feature in it ? You are adding restrictions over yourself and making your task more harder than it needs to be.\nI have seen overkill solution for famous java EE perm-gen exception - Java EE containers will eventually crash when you deploy WAR files multiple times, because loaded jars and classes are not garbage collected. The proposed solution will look to your WAR file, process classes and suggests how to bent your code to actually to prevent it. It seems useful. But do you really need WAR deployment ? Do you need full container ? If not, what about embedding container in your code ? Run simple Main class that starts containers in embedded mode. From this moment you have no problems with perm gens!\nThe simplest solution to any problem is \u0026hellip; do not create problem at first\nIf it is problematic for you to properly structure monolith, micro-services will be hell.\nLooking for the reason why something fails on distributed production system is hell. You do not have single process that can be broken, but you have dozens or hundreds of them.\nTherefore writing micro-services without tests on every level is no go. Cover your code with unit tests and gradually write integration tests. You will soon realise why coding without them is bad idea. It will take you days to actually find out why something does not work !\nThe approach of micro-services is the optimisation. The rule of thumb for optimisation - do not optimise if you do not need to - also applies for micro-services. Do not use them blindly. They can help a lot but they can be nightmare if they are done badly.\nPs. I am looking for stupid discussion that will arise in time: micro-services are dead, long live monolith. The same fallacy I have seen with tdd is dead. The disappointment of developers that actually did not understand topic well enough to find out that nothing is silver bullet and everything has its good and week parts.\n","date":"20 April 2015","permalink":"/posts/microservices-does-monolith-need-optimalization/","section":"Posts","summary":"","title":"Microservices - Does monolith need optimalization ?"},{"content":"Programming is not entirelly about about coding, design and architecture. A great deal of it is also about people we are working with. I often realize that I struglle more with ego and than with reason, especially when it comes to senior developers with \u0026ldquo;their ways\u0026rdquo;.\nOne of a common problems you can anccounter is problem of a screwdriver. You have to attach the schelf with imbus screws, but you do not own propper screw driver. What whould you do? Would you use impropper one ? No, you would go to the shop and obtain propper one. It is logical.\nBut often you have to deal with something different. You have a bunch of programmers that do not see why you need this different solution when \u0026ldquo;impropper\u0026rdquo; screwdriver worked just fine yesterday. You are trying to explain why, but it is not accepted, for many odd reasons, like \u0026ldquo;I dont belive it will work better\u0026rdquo;. Of course there are many resonable problems with new technology, you have to mitigate risk before. But sometimes problem is not in a technology or bussiness but in people themselves, more precisely in their egos. You silently say jeeesh, and fight your strugle because you know where it would end other way. You would cripple a screw. And any futre change will be nearly impossible.\nIf you like your job, you would definitely avoid this even though you would become black sheep. Your main motivation is product itself. You know that any change on diletant design would be costly and drasticall, therefore it is impossible te get rid off it completely. Making change hard is the guaranted path of doom.\nThe only constant thing in programming is change.\nI have ancountered this \u0026ldquo;back-pressure\u0026rdquo; numerous times with TDD, but absolute peak of resistance was reached when I have refused to use \u0026ldquo;inhouse framework\u0026rdquo; that was someone else toy, developed for years. I had become that bad coleague that has taken someone toy. Something mindblowing for me that time.\nAnd I have seen this nearly everywhere where something (framework, library, tooling) was developped for some time. The lack of will to change favourite tools to propper ones, fitting more to current situation.\nFor several years I was the lone C programmer in that company. I thought that everybody should be programming in C. But I could not convince any of my brethren to start programming in C. Does anybody have that problem ? With something like test driven development or agile practises ? The most common questions that people would ask me when I do lecture on TDD, is how can I convince anybody else to do this. And the answer to this question is you can not. Robert Martin\nSomehow I could not understand why so smart people are able to do such bad decisions. The journey for this answer has taken me far from my expertise domain that is programming, to psychology, namely to Dunning-Kruger effect.\nTheir initial test study was case of bank robberer McArthur Wheeler:\nIn 1995, McArthur Wheeler walked into two Pittsburgh banks and robbed them in broad dayligh, with no visible attempt at disguise. He was arrested later that night, less then an hour after videotapes of him taken from surveilliance cameras were broadcast on the 11 o\u0026rsquo;clock news. When police later showed him the tapes, Mr. Wheeler stared in incredulity. \u0026ldquo;But I wore the juice\u0026rdquo;, he mummbled. Apparently, Mr. Wheeler was under the impression that rubbing one\u0026rsquo;s face with lemon juice rendered it invisible to videotape cameras.\nThe experiment was done on Cornell University undergraduate students. They were tested on varius topics like humor, logical reasonig and english grammar. Then they had to write estimate of theirs abilities. How successfull they were compared to the others. They did not have a clue about the results of the others. It was only their guess.\nIn their famous paper Unskilled and unaware of it they argued that:\nPepole are incompetent in the strategies they adopt to achieve success and satisfaction, they suffer dual burden: Not only do they reach errorneous conclusions and make unfortunate choices, but their incompentence robs them of the ability to realize it.\nTop performers underestimate themselves, under performers overestimate their competence. Students with the lowest score (bottom quartile) has overestimated their results Students with the highest score (top quartile) has underestimated their results Estimates of top quartile students have improved when they were confrontated with work of the others. (They checked tests of other participants.) Students from bottom quartile have problems to realize their incompetence even after the confrontaition with results of the others. Compentent do not know they are compentent and incompetent do not realize their incompetence.\nAnd sometimes this is the exact siuation you are facing. Sometimes you work with people with superior skills but they do not use them fully because they underestimate themselves. I have heard many times we are not big enough, we cannot compete with gigants like google or facebook. You somehow understand them but then you realize that big teams do not work and any superior feature you are seening these days was made by the small teams, same as you have. If you have skillfull people why not use their full potential, take the risky way and go to the uncharted waters ? Nevertheless, this gives you the ability to strech your capability to a point beyond competition.\nPeople tend to do what are they used to, programmers are no exception of this.\nMany times you encounter colleaguaes with zero motivation. They are smart, full of potential but they do not believe in the project. This aspect is underestimated. We programmers like to focus on technological aspect of the project and underestimate everything else. They may seem inferior to the logical problems we love to solve. I have thought that alhorithms are the hardest part in software development process, but actually they are not when you are working in a team. Small things like motivation are absolutelly essential when it comes to the inovations. Motivation gives you the simple answer why to actually bother and learn new things and not use something that has worked \u0026ldquo;just fine\u0026rdquo; yesterday.\nIt is leathal when you use tools of yesterday today to invent something what will be successfull tomorrow.\nYou have to focus on the idea itself and do not distract yourselves by figuring out how to bend inappropriate tools, it may be your favourite library, framework, programming language, operating system, database system, cloud service. I have seens transition of file storage from openstack swift to mongo grid fs, because we have found out that swift is reading from disks all time to chesum stored files and garant files consistency. This has lead to disks failures more often than we would like to anticipate. Hardware is really costly and I did not mention delays due to logistics - someone has to buy disks and install them. Grid seems fine from the beggining, any nonstop disk checksums, until it starts to loose data due to software bugs. Try to explain this to your customer :). Then you realize that cloud storages like amazon offer you storage as service with no additional costs than monthly fee. You realize that storing data in big quantities is not as easy task as it was seen at the beggining. Storing data is not your core business (this bussines has been already dominated). From this point of view migration to amazon is very attractive. But it depends on your business, you will overpay if you are storing a few GB and storing a halph of the internet is more expensive than the custom solution. Therefore you should now your tools perfectly and also explore other possibilities. Maybe your problems are already solved and you are reinventing a wheel.\n","date":"27 November 2014","permalink":"/posts/design_pattern_screwdriver/","section":"Posts","summary":"","title":"Design Pattern: Screwdriver"},{"content":"","date":null,"permalink":"/tags/engineering/","section":"Tags","summary":"","title":"Engineering"},{"content":"","date":null,"permalink":"/tags/people/","section":"Tags","summary":"","title":"People"},{"content":"","date":null,"permalink":"/tags/soft-skills/","section":"Tags","summary":"","title":"Soft Skills"},{"content":"","date":null,"permalink":"/tags/caching/","section":"Tags","summary":"","title":"Caching"},{"content":"","date":null,"permalink":"/tags/functional/","section":"Tags","summary":"","title":"Functional"},{"content":"If you are programming for some time and you still not suffer from burnout syndrome, you definitely recognize that our industry is one of the most changing and adapting industry. On daily basis someone come with the new way of programing - new style, new paradigm, new revolutionary language or library. One would say that this has something in common with infancy of programming. If we compare programming with some other engineering industry like construction, we will see that ages helped it mature. There is no debate how to build the walls. They are straight. Corners are at the straight angle. Math from Pythagoras and Aristotelian is now common knowledge of high school laborer. Even though this knowledge was originally meant only for the highest philosophical debates about celestial bodies. The fact is that programming is is still young. Its like young wine - you do not know what to think about it. Its full of these endless debates. One of the hottest is functional programming.\nThe renaissance of functional languages started somewhere after new millennium with Scala (2003), Clojure (2007). The main motivation behind them is to handle problems of big system and clouds. It turns out that it is harder and harder to maintain big systems as their code base grows. New features pop often and every time it is harder to implement it. There was written a lot about this topic in classic imperative languages - it is solved with libraries model (dll, osgi). I was eager to discover what this new fresh breeze of functional programming has to offer.\nState #Out of the tar pit is one of the classic functional papers on this topic not written in 80\u0026rsquo;s :-). I liked tone of the the paper even more - authors did not need another alphabet and prefixed variables to express their thoughts. It is very simple to read :)\nLets start with simple request handling\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 public static ThrealLocal usersCache = new ThreadLocal(); public void handle(HttpRequest req, HttpResponse res) { String task = request.getParameter(\u0026#34;task\u0026#34;); String userName = req.getSession().getAttribute(\u0026#34;userName\u0026#34;); cacheUser(userName); try { doAuthorization(req); doTask(task); doOther(); } finally { usersCache.evict(); } } private void cacheUser(String userName) { if (userName != null) { User u = database.loadUser(userName); usersCache.put(u); } } public void doAuthorization() { User u = usersCache.get(); //authorize user ... } public void doTask(String task) { //do task if (task == \u0026#34;news\u0026#34;) doNews(); ... //log action User u = usersCache.get(); log(u+ \u0026#34; performed task \u0026#34; + task); } This naive pseudo code loads user from database, caches it in thread-local and runs the authorization followed with task execution. User info is cached in a static variable so it can be used from every part of the application. What is the problem with this code ? In this simple example nothing. You can easily postulate that request is always handled for single tenant identity (user always belongs to single organization in single request). But there is something treacherous. It is not visible on the first sight. Problems begins with code evolution. Our product and code is switching to multitenancy. Single user has different identity and different set of rights in every tenant (organization).\n1 2 3 4 5 6 7 private void cacheUser(String tenant, String userName) { if (userName != null \u0026amp;\u0026amp;| tenant != null) { User u = database.loadUser(tenant, userId); usersCache.put(u); } } Our first naive implementation would like this. Load also tenant and create user identity and cache it. Done! Seems still ok until your boss comes to your desk and gives you a task providing all news from all user tenants. And now you have a problem :). News task is the violation of basic postulate we mentioned before. (User is in single tennant in request). Ok do not worry you have to bent it somehow.\n1 2 3 4 5 6 7 8 9 public void doNews() { User u = usersCache.get(); String[] tenants = u.getTenants(); for (String t : tenants) { cacheUser(t, u.getUserName()); //load news ... } } You change cached value for every tenant and you are done! It seems fine until boss came again. And your code is more and more infested with knowledge about caching. The main problem here is management of state (usersCache) and propagation scope (static variable) of this state - it is accesible everywhere. Every part of code that is working with this value has to know that this is cached value for single tenant identity and has to work with it carefully. With bigger and bigger systems you would have more and more of these postulates across your code and your work will turn to constant solving of Rubik cube :) You will spent more and more time on whether you did not break any of those postulates until it will be unmaintainable. The same thing holds also for our tests. Your tests will have to test more and more obscure situations when your code is in that good or that bad static state and do not forget cleanup afterward.\nThis is the main idea of presented paper. Do not share unnecessary state (accidental state) across your code and make all your business logic functions \u0026ldquo;pure\u0026rdquo; - this means those functions do not depend on any state. All input are method variables and computed value is not stored \u0026ldquo;somewhere\u0026rdquo; but returned. This approach has a huge advantage.\nPure functions are easily tested.\nState again #As we mentioned before maintaining state can become hard when it is not managed. Functional languages provides a way how to solve it. To be more precise they make it much more harder to for programmer than just assign statement in imperative languages. Functional languages prefer pure functions, but to construct some real world application you need to maintain state in your application. The question is how.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ;; managed references (def bob (ref 238)) (def alice (ref 99)) ;; transactional \u0026#34;money move\u0026#34; (defn moveMonevy [source dest amount] (dosync (if (\u0026gt;= @source amount) (do (alter source - amount) (alter dest + amount)) (throw (Error. \u0026#34;Out of money :-0 !\u0026#34;))))) ;; make a money move (moveMonevy bob alice 65) @bob =\u0026gt; 173 @alice =\u0026gt; 164 Clojure, f.e. uses software transactional memory (STM) to handle state mutation. For this purpose it provides language primitives like ref, agent and atom. They have different semantics. In case of our example ref is demonstrated. It is a pointer to current value. You cannot change ref outside of doSync block otherwise you will get an error. You can only mutate state in transaction like manner that is provided with doSync. It guarantees you also the atomicity - money will be transferred from one account to another or it will fail after it reaches some threshold of retry. Yes, it retries ! Therefore mutation function has to be pure, without side effects, so it can be executed again. For more details I recommend brilliant presentation from Rich Hickey (author of Clojure)\nYet, another paradigm #If you look to any usable functional language you can divide it to two parts. Something that is pure functional (pure functions) and some kind of mechanism how to manage state. This are f.e. monads or STM. It is a mechanism how to handle state in a sane way. And from this point of view you can characterize it as a restriction over state. You do not propagate state as you want but you write your code to hold the paradigm. Such restriction is not only bound to functional languages. To demonstrate this switch to another paradigm, OOP. OOP is restriction over virtual functions. In case of ANSI C that is strictly procedural, OOP paradigm can be applied, but you have to handle it by yourself. You have to construct and pass proper function pointer. There is no language support for polymorphism and objects inheritance. You have to construct and use it by yourself. In same manner functional state paradigms can implied to imperative language. One could argue that this would look horrible. And it surely is horrible if it is done incorrectly. To overcome this Scala language was designed and Java made a first step to gain extra power and to remove unnecessary code of anonymous classes with introduction of lambdas. To be clear Scala is not functional language and it is not also OOP language. It can be tempting to think the opposite, but as it was mentioned before. Paradigm is the restriction. If you have state in objects you are breaking functional paradigm and if you have set of functions that are not bound to any object you break object oriented paradigm. It is a hybrid solution. So you are not restricting power but adding even more. So be careful. Power corrupts. And this new and popular language does not save you from making even bigger mess.\nDoes Multi Core Tend to be Functional? #One could argue that functional programming languages will soon or later replace imperative programming languages, because it brings restriction over state mutation. It forces you to write most of your code immutably and the only only necessary state is written with help of well defined abstract structures (ref, agent, atom, monads). Well, what is problem with that ? On the first sight nothing again :) , but \u0026hellip;\nAbstraction works well until it doesn\u0026rsquo;t.\nWhat I mean is performance. What would you do if you discover that your superior product that was written according to the newest trends in Clojure is not performing as you wish? Performance is one of your top priorities. If you are writing messaging system for trading, it is crucial to buy and sell shares with the lowest latency possible. You will find out that cpu contention is your real problem now. You will find out that threads are blocked an they are waiting in the queue instead of running at the maximum capacity of your multicore machine. You will find out that some of those language primitives of Clojure are written with JVM blocking synchronization primitives that requires context switching and require operating system to decide the final order of the threads. Do not forget garbage collection, there is still JVM underneath :).\nThere are several ways ho to solve it. And they do not depend on whether your language is functional or not. On of them is disruptor - approach of LMAX trading system. Their approach is simple and elegant. They use just single thread for logic so they do not use any synchronization primitives at all. They use one other thread for non-blocking io handling. The Careful reader would now argue that you still need blocking operation for passing request object from input thread to logic thread. Indeed this is the tricky part. Here comes disruptor for remedy.\nDisruptor is eventual consistent circular buffer of fixed size with single writer and possibly multiple readers.\nCrucial part here is single writer requirement. In case of single writing thread you can use lazySet function of atomic that mutate integer and do not block at all - it is eventually consistent. The other important detail is fixed size. It is the solution for garbage collection. Do not produce garbage at all. You should allocate everything at the beginning and then reuse.\nNow every functional geek should scream - reuse. It breaks the basic paradigm - immutable structures. It is true that you do not do this on daily basis. You have chosen functional language exactly to forbid this. You should not do this in program written with functional paradigm until you have a good reason and performance is such reason. For example Apache Storm is project written in clojure. But with version 0.8 they have switched their intert-hreads communications from blocking queues to disruptor.\nThe internals of Storm have been rearchitected for extremely significant performance gains. I\u0026rsquo;m seeing throughput increases of anywhere from 5-10x of what it was before.\nIn some cases you have to break paradigm to gain extra speed. Do not be restricted with paradigms if you have a good reason. But be careful.\nFunctional, the only way ? #Lets rewrite all our code in functional language! It is the only way ? Haskel will redeem us !!! Of course not. You can write crappy code in any language :).\nAfter my enlightenment with \u0026ldquo;functional way\u0026rdquo;, I see the KISS rule in different light. Infestation of code with accidental state is something that every developer should think about and evict from his project.\nSo be or not to be functional. It depends. The reason is still the same. Programming language is not matter of taste. It should be the best solution for your business cases. If you choose imperative language you will have to be careful about state. It is same responsibility as with self-managing of memory in C. You are given big power and also big responsibility. People are usually bad at this. That‚Äôs all.\n","date":"31 March 2014","permalink":"/posts/out-of-the-tar-pit/","section":"Posts","summary":"","title":"Out of the Tar Pit - The Good Parts"},{"content":"","date":null,"permalink":"/tags/state/","section":"Tags","summary":"","title":"State"},{"content":"","date":null,"permalink":"/tags/anti-patterns/","section":"Tags","summary":"","title":"Anti Patterns"},{"content":"Cometes is a Latin word with meaning \u0026ldquo;wearing long hair\u0026rdquo;. Aristoteles used it for description of stars with the long hair, for comets.\nComet is the collection of techniques based on HTTP, for processing of long living connections.\nLet‚Äôs introduce a typical comet application - chat. As a client, you post messages and expect responses as other users respond - in the real-time. You want see whether user is typing a message and his status (online, offline).\nComet is responsible for transfer - from client to server and from server to clients. In most cases single long living connection is established to server and when event occur a message is sent back and the connection is closed. Client then immediately establishes new connection. This way it is possible to emulate client to client communication.\nYou can find comet inside many popular web applications.\nGmail uses it for notifications, for chat. Facebook for everything :). Google Doc for real-time multi user editor. First attempts in 1996 #Comet is old technology. First time it was introduced in 1996 with Netscape. This was the first time when these two notions were introduced:\nserver push - server is sending messages to clients client pull - client is registering for messages Example request:\n1 GET http://www.w3.org/pub/WWW/TheProject.html HTTP/1.1 Response:\n1 2 3 4 5 6 7 8 9 10 11 12 13 Content-type: multipart/x-mixed-replace;boundary=ThisRandomString --ThisRandomString Content-type: text/plain Data for the first object. --ThisRandomString Content-type: text/plain Data for the second and last object. --ThisRandomString-- Client pulls request from server for some image or html page. As the server pushes response, content of DOM element is replaced with this new value (there can be delays between pushes).\nSupport in browsers is tragic, only firefox supports it fully.\nComet browser support. Why Comet was not globally adopted? #Comet was introduced in 1996, but why it was not globally adopted? The response is simple, there was no demand. Let‚Äôs realize what kinds of web applications were modern that time? Simple HTML pages with few images and linked with hyperlinks. Main case for comet was to show image advertisements - it is really only tiny bit of his potential.\nWhy it is good time now for real-time? #What has change since 1996? What is the reason for you to consider whether it is time for change? Web application has to show more and more real-time changes, to show graphs, implement chat, browsers notifications, client to client communication. On the other hand you do not want to refresh whole page again and again. This way page can become unusable. The send reason is that browsers now globally support more comet methods and comet is much easier to implement.\nTransport: Polling # Polling as data transport. It is not a real comet technique. (It does not have \u0026ldquo;long hair\u0026rdquo; :) ). It simulates real-time interaction by flooding server with sequence of requests. Server immediately responds with message or with no message. However the simplicity of technique is paid by lot of disadvantages:\nIt does not scale - with more clients your server will lag. It is unusable with very low number of clients (your DDos your own server ;) ) It is ineffective - every request has HTTP overhead. In most cases there is no message, nothing is transferred except HTTP headers. In small scale it is negligible, but in large instances it is an issue you should consider. Transport: Long-Polling # Long polling as data transport. The difference between polling and long polling are \"long hair\". Clients pull for messages and server pushes only when some message occurs or connection times out. This is more efficient way of doing comet and it is the most widespread technique, because it works on every browser even thou more fancy methods are avaible. Problematic is number of clients. When your server holds thread for every request until response, number of clients is limited by maximum of threads that can your server handle simultaneously. This is well known C10K problem. Hopefully this problem was overcame recently in Java with Servlet 3.0 JSR 315 specification where request thread can be returned to pool to handle other requests but socket is left open and waiting for event. Response is written from event thread.\nTransport: Streaming # Streaming as data transport. It is like long-polling. But server will push response to the same connection multiple times. Connection is reused for more messages. It can be done by creating XHR request and registering handler.\n1 2 3 4 5 6 xmlhttp.onreadystatechange = function() { if ( xmlhttp.readyState == 3) { //data has arrived //process chunk } } Server will respond with some data structure that can be deserialized. Data may be received by chunks. Program has to demultiplex and fire event to application when whole message is received.\nThis does not work in IE :( - IE do not return chunks, only whole message when connection is closed. This can be hacked, but it is more tricky. You can bend IE by sending back JSONP - script tags with JavaScript and JavaScript can be executed on the fly in iframe. You can read more here and here. Transport: HTML 5 API #Previous techniques were mostly hacks. They bend browsers in a manner they were never meant to be used. From the beginning HTTP was considered to be one way protocol and now we need to make it two way protocol. Preceding solutions works on all browsers but they do not work as we would like. HTML 5 comes with help. It brings JavaScript API to make comet easier to implement.\nAPI: Event Sourcing #It is standartized streaming. Client pulls and server pushes messages to the same connection.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 if (!!window.EventSource) { var source = new EventSource(\u0026#39;stream.php\u0026#39;); } else { // Result to xhr polling :( } source.onmessage = function (event) { alert(event.data); }; source.addEventListener(\u0026#39;add\u0026#39;, function(e) { if ( e.readyState == EventSource.CLOSED ) { // Connection was closed. } }, false); Browser handles connection - when connection is closed, browser automatically reconnects.\nAPI: Websockets # Websockets as data transport. After 20 years evolution of web we have finally realized that we need to rediscover what we already had in C for a long time - sockets. There is no extra magic. Client pulls for changes, server parses headers and use TCP connection directly. This connection is bidirectional. It is TCP socket for the web\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 var connection = new WebSocket(\u0026#39;ws://go.to/stream\u0026#39;); connection.onopen = function () { connection.send(\u0026#39;Ping\u0026#39;); // Send the message \u0026#39;Ping\u0026#39; to the server }; // Log errors connection.onerror = function (error) { console.log(\u0026#39;WebSocket Error \u0026#39; + error); }; // Log messages from the server connection.onmessage = function (e) { console.log(\u0026#39;Server: \u0026#39; + e.data); }; RFC 2616: A single-user client should not maintain more than 2 connections with any server or proxy.\nEvery browser has limit how many parallel connections can establish. Therefore comet is often implemented by single connection to the server and all messages are multiplexed to this single connection. If it is used more than limit, request will simply wait until some connection is freed. This can bring unwanted lags.\nNumber of paralel connection. Server side #In world of java, comet was properly introduced by Servlet 3.0. It brings the possiblity to make comet without hacks via standart API.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @WebServlet( name=\u0026#34;myServlet\u0026#34;, urlPatterns={\u0026#34;/slowprocess\u0026#34;}, asyncSupported=true) public class MyServlet extends HttpServlet { public void doGet(HttpServletRequest request, HttpServletResponse response) { //store context for later usage AsyncContext ctx = request.startAsync(); ... //you can register connection listener ctx.addListener( new AsyncListener() { public void onComplete(AsyncEvent ae) {} public void onTimeout(AsyncEvent ae) {} public void onError(AsyncEvent ae) {} public void onStartAsync(AsyncEvent ae) {} } ); } } API is pretty simple. You mark servlet to support long request (asyncSupported=true). You create AsyncContext from request. You store it somewhere to memory. Method doGet is non-blocking. It finishes properly but socket is still opened. When event occures you pick context and writes response as for usual request.\nWeb applications are evolving. #From simple HTML pages to single page applications. Applications became more social, more user friendly, more useful. Comet is a part of paradigm shift that brings web application to the new level even thou HTTP was not designed for it. A lot of people have realized it. Web is no longer static set of CSS and HTML. It is a means how to easily deliver apps to users written on a single platform. It does not matter whether you are using Linux, Windows, Mac, Android, IOs. You can run your apps on all of those systems, written once in JavaScript. Many have tried before but failed: Java Applets, Adobe Flash, Microsoft Silverlight. To be honest current state of web programming is far from ideal. Making something cross-browser is a pain. Not all browsers supports HTML 5 fully now, but all browsers make effort to fulfill it. This brings less hacking and more focus on business value.\n","date":"27 December 2013","permalink":"/posts/comet/","section":"Posts","summary":"","title":"Comet: trendy and hairy"},{"content":"The main addition of Java Servlet 3.0 spec was the introduction of Commet techniques. You can handle long request without blocking. You could have done this before but you had to become platform specific. You depend on servlet container specific API. Servlet 3.0 brought standard API, so this is no issue anymore. What is the issue is the state of those containers. I can only judge two of them that I have been heavily using in the past. To be more precise, Tomcat and Jetty. Servlet 3.0 was a big change. Now out of the blue synchronous request model was changed and servlet 3.0 change had to be transferred to the new code-base without global rewrite current code. The result was turmoil. I would demonstrate it on my case with Tomcat.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public class AsyncLongRunningServlet extends HttpServlet { protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { //(1) create async context AsyncContext asyncCtx = request.startAsync(); //(2) register listener asyncCtx.addListener(new AsyncListener() { ... onComplete(AsyncEvent asyncEvent) .... ... onError(AsyncEvent asyncEvent)... ... onStartAsync(AsyncEvent asyncEvent)... ... onTimeout(AsyncEvent asyncEvent)... }); //(3)set timeout asyncCtx.setTimeout(9000); //(4) store it somewhere in the heap for your purposes .... //(5) return from method - do no block this thread till response return; } } This is a typical scheme of asynchronous servlet, you can find it in every demo example:\nYou make request asynchronous by calling startAsync You register listener for various state changes. You set request timeout You store it to some global list, executor, whatever you business logic requires. This is some kind of waiting list, where requests waits for some event (f.e.: message was sent and it is propagated to clients in case of chat application) You should not make any long operations here like waiting etc. You just return. Method ends, thread is returned to pool but socket is still opened. It seems easy, simplistic but it hides some weak point that are not visible from the first sight.\nI was in charge of implementing of Pub-Sub feature in a product. So part of the application was JSON-RPC like API, with thread per request model and the second one was notifier. Servlet 3.0 API is well documented, so first implementation was done quickly. But out of the blue responses become corrupted. Sometimes you get response that contained JSON response with a tiny part of some other JSON or a part of image. I have spent a lot of time with debugging with no result. This mistake was unfortunately occurring in the production with some amount of traffic that I could not locally reproduce. In the end I wrote to Tomcat forums and I got this response:\nDid I miss some critical lesson about flushing async responses ? HttpServletRequest and HttpServletResponse objects a) are not thread-safe b) are cached and reused for subsequent incoming requests. In fact Tomcat (and Jetty as well) recycle request and response objects. They replace only internals. This is done to avoid unnecessary garbage collection. But on the other hand this breaks the isolation of request and tons of unpredictable bugs can occur. Life without isolation is really painful, because if you make a bug in one place it corrupts non-deterministically other places that are fine.\nFrom feature implementer point of view point 4 brings problems with isolation. You can forget to properly clean structure. Yes, you make a bug. It happens, but finding it is a pain - it is non-deterministic. From my point of view servlet containers are not safe enough. Every code that is used for multithread programming should have as much immutables as possible. It makes code more readable and simpler. And certainly they should NOT recycle requests without your direct intent.\nSo out of frustration I have started to dig up in the source-codes. I wanted to debug it and with luck patch it. No luck here. Sources are overcomplicated due to long maintenance without rewrite and they are full of fuzzy bits like this:\n1 sk.attach(attachment);//cant remember why this is here Codebase is improving over time. Jetty was completely rewritten in version 9, so places like this are less common. You do not find \u0026ldquo;cant remember why\u0026rdquo; places in Tomcat 8 NIO connector implementation. But the main problem, the request recycling is still present in both web containers.\nThis all brings me to the ultimate question:\nDo you really need servlet container? #Do you need all servlet fancy features like hot deploy, wars class-loader isolation, life cycle. It is not your case, just HTTP service? Tomcat / Jetty are heavily used, but do you need all of their power and abstraction? They were precisely designed to fulfill multiple demands. I am just asking whether these cases are yours. If not, why just not to use \u0026ldquo;plain old embedded HTTP\u0026rdquo; ? Why to use servlets at all ?\nThere are bunch other possibilities that were written from scratch for non-blocking communication and they are stable (Netty or Vertex).\nIf you can choose technology I would recommend you to avoid Servlet container for comet applications until you really need them (You can clearly answer the question: why it is actually helpful?). If you can not, be prepared to solve tons of problems of too blury abstraction and caching. And as we know caching is hard.\n","date":"27 December 2013","permalink":"/posts/not_only_servlet/","section":"Posts","summary":"","title":"NO (not only) Servlet"},{"content":"","date":null,"permalink":"/tags/premature-optimisation/","section":"Tags","summary":"","title":"Premature Optimisation"},{"content":"","date":null,"permalink":"/tags/rewrite/","section":"Tags","summary":"","title":"Rewrite"},{"content":"","date":null,"permalink":"/tags/servlet/","section":"Tags","summary":"","title":"Servlet"},{"content":"","date":null,"permalink":"/tags/transport/","section":"Tags","summary":"","title":"Transport"},{"content":"","date":null,"permalink":"/tags/web/","section":"Tags","summary":"","title":"Web"},{"content":"","date":null,"permalink":"/tags/languages/","section":"Tags","summary":"","title":"Languages"},{"content":"What is the best programming language? I have spent many hours in this endless debate and the conclusion was that for many programmers it is matter of taste.\nToday the sexiest language is JavaScript \u0026amp; Node.js platform. Tons of new project are popping up like a fresh breeze. The new zeitgeist is \u0026ldquo;all in JavaScript\u0026rdquo;. It is easy, it is fast and event-loop will redeem your project. You write your client/server code in a single language. So from now you just need JavaScript coders.\nGithub repositories language chart. Really? It is really matter of taste? Of course not. Programming language should always be matter of your current circumstances, workload you are solving.\nIf you want to scale, It is a bad idea to write database drivers purely in JavaScript. Think first about garbage objects that this will produce. At low scale it is ok but soon you will reach technology limit. Then you can choose whether to buy new machine or optimize your code. It is not obvious choice. You can use well written low level driver written in C. You will wrap it with JavaScript and create npm package. Everything seems fine again until bugs pop out and you cannot debug to linked C library from JavaScript. Your programmer is speechless and he want you to find someone that fixes it. You can hire some C freak or you can just buy new and new machine. Sometimes new machine is quite enough, sometimes TCO (total cost of ownership) is bigger.\nWrite your code once - client and server in a single source base. It is a myth. I have written in Javascript for a few years and there were numerous cases when IE sucked, when Firefox had some bug, when Chrome went crazy. On the other hand you want to support the widest range of browsers, so you have to ify (add if) your code. You have multiple bent places here and there to overcome that or other browser weak part - it is better not to share this. Maybe in future this will be fixed but now this sucks.\nWheel of reinvention - Using JavaScript everywhere is trend. It may seem that they solve a new set of problems. They do not. For example sockets are known from beginning of network communication, but in JavaScript it is a new HTML 5 addition with new name WebSockets. In fact JavaScript greatly solves just a single problem and this is application distribution. No complicated installation, but you can bring similar user experience as native apps with one click. Therefore browsers are becoming minimalistic, chrome is just a single toolbar, so there is more place for your application.\nJavascript is oure evil - This is also common opinion. JavaScript is slow. You have to download MB of javascript to render DOM. You should instead render html on server. On the other hand there are multiple reasons why you do not want to do this. When you divide presentation layer from server business logic you can develop both sides separately. You specify API between them f.e. via apiari.io and then you can develop in parallel. Second reason is even more important. You can make your applications rich. It is not set of isolated pages anymore. With HTML 5 JavaScript you can create hell of user experience. This is a paradigm shift. Internet connection is fast so you can transfer MB of JavaScript to your single page app. You often do this only once and then internal cache of browser will return stored code without download from server. Javascript is very poweful tool, but if you give knife to small child, he will cut himself.\nAlways think about your problem first. What are your cases? Will language X solve some of them? It should not be matter of personal prefferences, feelings. I think \u0026hellip;, I hope \u0026hellip; are often signs of assumtions and bias. you should avoid it in your project. You can easily find someone with different opinion in your team. Endless debates will make you tired without any gain of business value. You loose time and money. Software is risky business. Can you afford to burn your opportuninties instead of delivering value ?\nThere is no best programming language. There is optimal language for your job.\n","date":"17 November 2013","permalink":"/posts/religious-issue/","section":"Posts","summary":"","title":"Religious issue: The best programming language"},{"content":"One of many things that I had to learn when I graduated from college was to make things done. I do not say that I have not successful projects before but I spent lot of time to search the best way.\nI had a habit to figure out what technology to use before I have my use-cases. It is kind of habit of programmers that they like to do make their own frameworks for the things they do. They glue some set of classes and they are eager to use them everywhere.\nThey tend to use other frameworks that are popular in community. Actually they use tons of them. It is nothing bad until you can clearly answer what problem they solve, what are the positives and what slows you down. Without these answers it is only blind religion and then comes inevitably time when you will bend your code for purposes of framework limitations.\nIn the end, maybe you do not need those sexy frameworks.\nI had other assignment. I had to write set of diagrams to present dependencies and structure of new protocol I was implementing. I have spent a lot of time to find best tool for drawing diagrams and I did not make it in time. Then I realise I do not need it at all. I just need my whiteboard and markers and do the job fast and dirty. Then I used Inkscape for fast sketch. The task was done in a few hours instead of days.\nDo not try to figure out the best way because you do not see the whole picture. Just try to solve your problem.\nA things you know\nthings you know things you know that you don\u0026rsquo;t know things you don\u0026rsquo;t know that you don\u0026rsquo;t know This picture displays normal order of knowledge we possess. Do not try to figure out the future. We are bad at this. Focus what is here, what is now. Base your actions on actual metrics and make decision based on them. Reasoning with \u0026ldquo;It is good to\u0026rdquo;, \u0026ldquo;I think\u0026rdquo;, \u0026ldquo;I hope\u0026rdquo; is a bad practise. You can easily stuck in blind corner of programming religion without creating any business value. Lets face it, business value is what actually generates money that we earn.\n","date":"6 November 2013","permalink":"/posts/lesson-learned/","section":"Posts","summary":"","title":"Lesson learned: Do your job instead of looking for the best way."},{"content":"It happened some time ago. Some of our servers went crazy. Traffic was in normal, but out of the blank CPU was 95%. It seemed to be a nutshell of a day. I looked for the culprit in stack traces with my colleagues and found this place in code\n1 2 3 public JSONObject createJSON(String raw) { return new JSONObject(raw); } Creating simple JSON object caused the infinite loop?! WOW. JSONObject uses internally java.util.HashMap. After some googling, there was a single culprit candidate - race condition.\nBut how it is posssible? There is no shared state. Object is created on the heap and it is not shared between threads. My mind was torn to two pieces. One was 100% sure that this is a race condition but the second one told me the oposite.\nOut of cheer frustration we started to look to the sourcecodes of JSON library. And we found it !\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 public class JSONObject { /** * The maximum number of keys in the key pool. */ private static final int size = 100; /** * Key pooling is like string interning, but without * permanently tying up memory. To help conserve * memory, storage of duplicated key strings in * JSONObjects will be avoided by using a key pool * to manage unique key string objects. This is * used by JSONObject.put(string, object). */ private static HashMap keyPool = new HashMap(size); public JSONObject(String source) throws JSONException { ... //cache parsed keys String pooled = (String)keyPool.get(key); if (pooled == null) { keyPool.put(key, key); } ... } } I stayed stunned for a while. A static map shared between all instances of JSONObject - the mind-blower of a day. This solved all our questions. This was the wanted critical section. We could not reveal it before because it happened only sometimes when HashMap internal threshold was reached (we did not use more than 100 JSON unique keys before :DD ). This bug was hopefully fixed in the library after half a year.\nThis is my personal encounter with anti-pattern \u0026ldquo;Premature optimization\u0026rdquo;. It caused whole day of searching, patching source codes and redeploying server instances. A lot for a simple sunny day\u0026hellip;\n","date":"31 October 2013","permalink":"/posts/straight-way-to-abyss/","section":"Posts","summary":"","title":"Straight way to abyss: Premature optimization"},{"content":"There is always one big obstacle in software development. Developers themselves.\nFirst you will realize that your team is not composed of top programmers. You will find out that there are numerous programmers with hard-coded way of doing things and OOP is for them new way of doing their job. And they are not used to it. They are smart, they love statics and they love them very much. And if you are not careful and you listen to them too much your code will turn to one monolithic rock that is pain to maintain, because everything is bound to everything via static references. Every change is harder and harder and it will inevitably come time when its heaviness will crush your project to abyss.\nSecondly, when you work with someone, it is important to have some code conventions. Now come the hard part. Programmers are really selfish people and it is hard to persuade them to use same coding standards. They do not like it. They will argue that they will make it much faster their way. This problem is similar to building dog-kennel:\nWhen politics are approving nuclear plant, it is decided in minutes. Nobody really knows how that complicated atom-splitting force works, so the plan will be approved without objections. But then they have to make judgement on dog-kennel placed in front of it. They will not agree in decades. Because everyone have different option about the proper colour.\nBefore introduction of any new technology you have to have overall agreement, to prevent two or multiple coding standards. At least try it :)\nThird. Developers are messy and make a tons of crap code. Every time I see this, the reason is most of the time the same. Then I always remember one story from Bob Martin\u0026rsquo;s speach at QCon.\nWhen we were young, 9 years old, we had a system for our bedroom: ‚Äúsomewhere on the floor‚Äù. Underwear was somewhere on the floor. Then we easily found it. Socks were somewhere on the floor, under the bed. We found them because we had good system. We knew where thinks were - somewhere on the floor. Then our mum came in. She started to clean the room out of cheer frustration because there was no way to know for 9 years old what the concept of order means. This does not change till you get 30 and get married. And then you realize when 2 people are using the same space, the system does not work so perfectly.\nI personally consider people relations the hardest part of programming. To make your project done you have to be sometimes loving and caring mother or sometimes ruthless dad when there are no other options. You have to find way to all members of your team. Find problem that blocks them from doing their task and solve it. Otherwise it harms your project, your goals your business.\n","date":"29 October 2013","permalink":"/posts/first-step-to-design/","section":"Posts","summary":"","title":"First step to design: It is always about people"},{"content":"","date":null,"permalink":"/tags/politics/","section":"Tags","summary":"","title":"Politics"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]